# WSI Classification with MoE-based Token Compression

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Production-ready PyTorch research codebase for **Whole Slide Image (WSI) Classification** using a novel **Mixture of Experts (MoE) Token Compression** mechanism.

## ğŸ¯ Overview

This project implements a deep learning pipeline for classifying Whole Slide Images using pre-extracted patch features from foundation models (UNI, CTransPath, etc.). The core innovation is the **MoE Token Compressor** that intelligently reduces thousands of variable-length patch embeddings into a fixed set of semantic tokens for classification.

### Key Innovation

**MoE Token Compressor**: Each patch is routed to expert slots via a gating network, aggregated with weighted pooling, and processed by dedicated experts. This approach:
- âœ… Compresses N variable patches â†’ K fixed semantic tokens
- âœ… Learns semantic clustering of tissue patterns
- âœ… Ensures load balancing across experts
- âœ… Outperforms standard attention-based MIL

---

### Key Directories

| Directory | Purpose |
|-----------|---------|
| `src/` | Core source code (models, data, utilities) |
| `tools/` | Training, evaluation, and data generation scripts |
| `configs/` | YAML configuration files for experiments |
| `examples/` | Code examples for inference and usage |
| `tests/` | Unit tests and verification scripts |
| `data/` | Dataset (features + CSVs, **not tracked in git**) |
| `outputs/` | Training results (**not tracked in git**) |

---

## âš™ï¸ Installation

### Prerequisites

- Python â‰¥ 3.8
- CUDA-capable GPU (optional but recommended)
- Pre-extracted WSI patch features (.pt files)

### Step 1: Clone Repository

```bash
git clone https://github.com/OzzyChen97/wsi-moe-classifier.git
cd wsi-moe-classifier
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```
---

### 1. Prepare Your Data

#### Data Format

Your data should follow this structure:

```
data/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ slide_001.pt
â”‚   â”œâ”€â”€ slide_002.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train.csv
â”œâ”€â”€ val.csv
â””â”€â”€ test.csv
```

#### CSV Format

```csv
slide_id,label
slide_001,0
slide_002,1
slide_003,0
```

- **slide_id**: Identifier matching the .pt filename (without extension)
- **label**: Integer class label (0, 1, 2, ...)


## ğŸ”§ Model Architecture

### MoE Token Compressor

```
Input: [Batch=1, N, 1024] (Variable N per slide)
    â†“
Gate Network: Linear(1024 â†’ K slots)
    â†“
Top-1 Routing: Assign patches to expert slots
    â†“
Weighted Aggregation: Pool patches per slot
    â†“
Expert Processing: Each slot refines features
    â†“
Output: [Batch=1, K, 1024] (Fixed K tokens)
```

**Load Balancing Loss**:
```
expert_importance = Î£ gate_probs per slot
CVÂ² = (std / mean)Â²
Total Loss = CrossEntropyLoss + Î» Ã— CVÂ²
```

### Complete WSI Classifier

```
Input [1, N, 1024]
  â†“
MoE Compressor [1, K, 1024]
  â†“
Mean Pooling [1, 1024]
  â†“
MLP Classifier [1, num_classes]
  â†“
Output: Logits
```

---

## ğŸ›ï¸ Hyperparameter Tuning

### Key Parameters

| Parameter | Range | Effect |
|-----------|-------|--------|
| `num_slots` | 32-128 | Compression vs. capacity trade-off |
| `aux_loss_weight` | 0.001-0.1 | Load balancing strength |
| `lr` | 5e-5 to 2e-4 | Learning rate |
| `grad_accum_steps` | 4-16 | Effective batch size |

### Recommended Configurations

#### Binary Classification

---


## ğŸ† Benchmark Comparison

We compared our **WSI_MoE** model against state-of-the-art methods on the **TCGA-BRCA** dataset.

| Method | Backbone | Accuracy | Data Source |
|:---|:---|:---:|:---|
| **WSI_MoE (Ours)** | **UNI** | **93.75%** | Real-world Evaluation |
| **ABMILX** | ResNet-50 | 95.17% Â± 5.82 | [ArXiv:2506.02408] |
| **GIGAP** | TransMIL | 93.97% Â± 3.88 | [ArXiv:2506.02408] |
| **UNI** | TransMIL | 93.33% Â± 3.50 | [ArXiv:2506.02408] |
| **CHIEF** | - | 91.43% Â± 4.52 | [ArXiv:2506.02408] |
| **CLAM** | ResNet-50 | 85.86% Â± 6.43 | [ArXiv:2506.02408] |

*All competitor results are cited from [ArXiv:2506.02408](https://arxiv.org/abs/2506.02408).*

### Latest Evaluation Metrics (Reported)

| Metric | Value |
|:---|:---:|
| **Accuracy** | **93.75%** |
| **AUC** | **0.9638** |
| Precision | 0.9375 |
| Recall | 0.9375 |
| F1 Score | 0.9375 |

**Confusion Matrix**:
```text
[[74,  3],
 [ 3, 16]]
```
*Class 0 (Normal): 77 samples, Class 1 (Tumor): 19 samples*

---


## ğŸ› ï¸ Troubleshooting

### Out of Memory (OOM)

**Solutions:**
1. Enable mixed precision: `--use_amp`
2. Increase gradient accumulation: `--grad_accum_steps 16`
3. Reduce model size: `--num_slots 32`
4. Use CPU: `--device cpu`

### Poor Performance (AUC < 0.7)

**Checklist:**
- âœ… Check feature quality (UNI > CTransPath > ResNet)
- âœ… Verify label correctness in CSV
- âœ… Tune hyperparameters: `--num_slots 96 --lr 5e-5`
- âœ… Increase training: `--num_epochs 100`

### File Loading Errors

**Verify:**
- CSV has `slide_id` and `label` columns
- Feature files named as `{slide_id}.pt`
- Feature format: Dictionary with 'features' key or direct tensor
- Feature dimension matches `--feature_dim`

### Training is Slow

**Optimizations:**
- Enable AMP: `--use_amp`
- Increase workers: `--num_workers 8`
- Use GPU: `--device cuda`

---

## ğŸ“š Additional Resources

- **Quick Start Guide**: See `QUICKSTART.md` for step-by-step tutorial
- **Project Structure**: See `PROJECT_STRUCTURE.md` for detailed organization
- **Example Configs**: Check `configs/default.yaml` for configuration templates

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“§ Contact

For questions, issues, or collaboration:
- **GitHub Issues**: [Report bugs or request features](https://github.com/OzzyChen97/wsi-moe-classifier/issues)
- **Email**: comfortableapple@gmail.com

---

## ğŸ™ Acknowledgments

- Built with PyTorch
- Inspired by Mixture of Experts literature
- Designed for computational pathology research

---

## â­ Citation

If you use this code in your research, please cite:

```bibtex
@software{wsi_moe_classifier,
  title={WSI Classification with MoE Token Compression},
  author={OzzyChen97},
  year={2024},
  url={https://github.com/OzzyChen97/wsi-moe-classifier}
}
```

---

**Happy Training! ğŸš€**
